# E8-S5 — Declarative `robots.txt` Generation

**Epic:** Epic 8 — DX & Reliability  
**Status:** Not Started  
**Priority:** P2 (Medium — foundational crawl-control capability for production sites)  
**Estimated Effort:** 1–2 days  
**Dependencies:** E8-S4 (website-scoped favicon support / first-class `Website` desired state)  
**Target:** `pkg/model`, `pkg/loader`, `internal/state`, `internal/db`, `internal/release`, docs  
**Design Reference:** PRD declarative-resource model + artifact-promotion invariant

---

## 1. Objective

Add typed, declarative `robots.txt` generation for `htmlctl` websites so operators can control crawler access without managing a hand-written file in the release output.

This feature must stay true to the product’s promotion model: `robots.txt` is generated deterministically from declarative website state during release materialization and is then promoted as an immutable artifact with no rebuild.

## 2. User Story

As an operator publishing a site with `htmlctl`, I want to define crawler rules in `website.yaml` and have `robots.txt` generated automatically, so crawl behavior is versioned, reviewable, and consistent across local render, staging, and production artifacts.

## 3. Scope

### In Scope

- Add website-level SEO/crawl configuration to `website.yaml`.
- Generate `/robots.txt` from typed website metadata.
- Support ordered crawler policy groups with:
  - user-agent matching,
  - allow rules,
  - disallow rules.
- Support deterministic default generation when `robots.txt` is enabled but no groups are provided.
- Persist robots configuration through apply/diff/release.
- Keep output deterministic and artifact-stable for promote.

### Out of Scope

- Raw text passthrough for arbitrary hand-authored `robots.txt`.
- Environment/domain-binding-derived `robots.txt` content.
- Runtime generation at request time.
- Automatic sitemap generation in this story.
- Crawl-delay, Host, or vendor-specific non-standard directives in v1.

## 4. Architecture Context and Reuse Guidance

- Reuse the `Website` desired-state foundation introduced by E8-S4. `robots.txt` is website-scoped metadata, not a page feature and not a generic static file.
- Do not route this through `pkg/renderer`; `robots.txt` is a release artifact, not page HTML. Generate it in the release/materialization pipeline alongside other release-level outputs, similar in spirit to OG image materialization but much simpler.
- Promotion constraints from E4-S3 still apply:
  - no rebuild during promote,
  - no environment-specific rewrite,
  - generated bytes must already be correct in the promoted source release.
- Therefore, any absolute URLs used later by crawl features must come from operator-declared website metadata, not live domain bindings.

## 5. Proposed Design

### 5.1 Website Schema

Extend `WebsiteSpec` with a typed SEO block:

```yaml
spec:
  defaultStyleBundle: default
  baseTemplate: default
  seo:
    publicBaseURL: https://example.com
    robots:
      enabled: true
      groups:
        - userAgents:
            - "*"
          allow:
            - /
          disallow:
            - /preview/
            - /drafts/
```

Suggested Go model:

- `WebsiteSEO`
  - `PublicBaseURL string`
  - `Robots *WebsiteRobots`
- `WebsiteRobots`
  - `Enabled bool`
  - `Groups []RobotsGroup`
- `RobotsGroup`
  - `UserAgents []string`
  - `Allow []string`
  - `Disallow []string`

### 5.2 Control vs Automation

Automation:

- If `robots.enabled` is `true` and `groups` is empty, generate a default allow-all policy:
  - `User-agent: *`
  - `Allow: /`

Control:

- Operators can define ordered groups explicitly.
- Output order must follow manifest/input order for groups and directives.
- No implicit blocking or vendor-specific policy should be added.

### 5.3 `publicBaseURL`

`publicBaseURL` is introduced in this story because future crawl artifacts need a canonical site origin that survives promotion unchanged.

Rules:

- must be absolute `http(s)`
- must not include query or fragment
- trailing slash normalization is deterministic

This field is metadata, not an environment binding. It represents the site’s canonical public crawl origin.

### 5.4 Release Output

When `robots.enabled` is `true`, release materialization writes a root-level `/robots.txt`.

Formatting rules:

- LF line endings only
- blank line between groups
- deterministic directive ordering within each group:
  1. `User-agent` lines in input order
  2. `Allow` lines in input order
  3. `Disallow` lines in input order

No sitemap line is generated in this story. E8-S6 may append it later when sitemap support exists.

## 6. File Touch List

### Files to Modify

- `pkg/model/types.go`
  - Add typed website SEO/robots structs.
- `pkg/model/types_test.go`
  - Add YAML/JSON roundtrip coverage.
- `pkg/loader/validate.go`
  - Validate `publicBaseURL`, robots groups, and directive values.
- `pkg/loader/validate_test.go`
  - Add robots validation coverage.
- `pkg/loader/loader_test.go`
  - Add fixture coverage for website SEO parsing.
- `internal/db/models.go`
  - Extend `WebsiteRow` to carry `HeadJSON`/`ContentHash` from E8-S4 plus new SEO JSON field if schema is split, or document that website metadata remains in a single `head_json`/`seo_json` payload.
- `internal/db/queries.go`
  - Persist/retrieve website SEO metadata.
- `internal/db/queries_test.go`
  - Cover website SEO persistence.
- `internal/state/merge.go`
  - Detect website SEO changes and persist them.
- `internal/state/merge_test.go`
  - Cover apply/update behavior for website SEO.
- `internal/release/builder.go`
  - Load website SEO metadata and materialize `/robots.txt`.
- `internal/release/builder_test.go`
  - Verify output bytes and presence/absence behavior.
- `docs/technical-spec.md`
  - Document website SEO config and generated `robots.txt`.

### Files to Create

- `internal/release/robots.go`
  - Deterministic `robots.txt` generation helper.
- `internal/release/robots_test.go`
  - Unit tests for formatting and ordering.

## 7. Implementation Steps

1. Add typed website SEO/robots config to `pkg/model`.
2. Add loader validation for:
   - valid absolute `publicBaseURL`,
   - non-empty user agents,
   - relative path-like allow/disallow directives,
   - bounded counts/lengths.
3. Extend website desired-state persistence so robots config survives apply/release/diff.
4. Implement deterministic `robots.txt` generation helper in `internal/release`.
5. Call that helper from release materialization when robots are enabled.
6. Update docs and fixtures.

## 8. Tests and Validation

### Automated

- Unit test: website YAML parses `spec.seo.robots`.
- Unit test: invalid `publicBaseURL` is rejected.
- Unit test: robots groups preserve input order.
- Unit test: `enabled: true` with no groups emits default allow-all policy.
- Unit test: disabled/missing robots config emits no `robots.txt`.
- Unit test: release output contains deterministic LF-only `robots.txt`.
- Unit test: apply/release roundtrip preserves website SEO metadata.
- `go test -race ./...`

### Manual

- Apply a site with explicit robots groups and inspect generated `/robots.txt`.
- Apply a site with `enabled: true` and no groups; verify default allow-all output.
- Promote staging -> prod and verify promoted artifact contains identical `robots.txt`.

## 9. Acceptance Criteria

- [ ] AC-1: `Website` resource supports typed `spec.seo.robots` configuration.
- [ ] AC-2: `spec.seo.publicBaseURL` is validated as an absolute canonical site URL.
- [ ] AC-3: When robots generation is enabled, release output contains a deterministic root `/robots.txt`.
- [ ] AC-4: If no robots groups are provided, the system generates a default allow-all policy.
- [ ] AC-5: Website robots configuration is part of desired state and survives apply -> DB -> release roundtrip.
- [ ] AC-6: Promote copies `robots.txt` as part of the existing immutable release artifact with no rebuild.
- [ ] AC-7: No raw passthrough or environment-derived rewrite path is introduced.

## 10. Risks and Open Questions

- **Risk:** operators may expect domain bindings to drive crawl URLs automatically.
  **Mitigation:** explicitly document that crawl artifacts use canonical website metadata, not environment bindings, to preserve promote invariants.

- **Risk:** too many non-standard robots directives expand scope quickly.
  **Mitigation:** keep v1 limited to `User-agent`, `Allow`, and `Disallow`.

- **Open question:** none blocking for v1.

## 11. Architectural Review Notes

> Added after codebase review. These notes resolve ambiguities and provide implementation-ready guidance.

### 11.1 Current State Baseline

- `WebsiteRow` has no metadata columns beyond `default_style_bundle` and `base_template`.
- E8-S4 adds `head_json` and `content_hash` columns in migration **005**.
- This story adds a separate `seo_json` column in migration **006**. Do not merge SEO config into `head_json` — they serve different concerns (head/favicon vs. crawl metadata) and must remain isolated.
- The OG image pipeline (`internal/release/builder.go` → `internal/release/ogimage/`) is the correct reference pattern for release-time artifact generation (single-shot, deterministic, no caching needed for robots.txt).

### 11.2 DB Migration (006)

```sql
-- internal/db/migrations/006_website_seo.go
ALTER TABLE websites ADD COLUMN seo_json TEXT NOT NULL DEFAULT '{}';
```

Add a helper method on `WebsiteRow` (parallel to `HeadJSONOrDefault()` on `PageRow`):
```go
func (r WebsiteRow) SEOJSONOrDefault() string {
    if strings.TrimSpace(r.SeOJSON) == "" {
        return "{}"
    }
    return r.SeOJSON
}
```

Update `GetWebsiteByName` (and any other website SELECT) to include `seo_json`.

### 11.3 Go Model Types

Add to `pkg/model/types.go`:

```go
type WebsiteSEO struct {
    PublicBaseURL string         `yaml:"publicBaseURL" json:"publicBaseURL,omitempty"`
    Robots        *WebsiteRobots `yaml:"robots,omitempty" json:"robots,omitempty"`
}

type WebsiteRobots struct {
    Enabled bool          `yaml:"enabled" json:"enabled"`
    Groups  []RobotsGroup `yaml:"groups,omitempty" json:"groups,omitempty"`
}

type RobotsGroup struct {
    UserAgents []string `yaml:"userAgents" json:"userAgents"`
    Allow      []string `yaml:"allow,omitempty" json:"allow,omitempty"`
    Disallow   []string `yaml:"disallow,omitempty" json:"disallow,omitempty"`
}
```

Extend `WebsiteSpec`:
```go
type WebsiteSpec struct {
    DefaultStyleBundle string       `yaml:"defaultStyleBundle"`
    BaseTemplate       string       `yaml:"baseTemplate"`
    Head               *WebsiteHead `yaml:"head,omitempty" json:"head,omitempty"` // added by E8-S4
    SEO                *WebsiteSEO  `yaml:"seo,omitempty"  json:"seo,omitempty"`  // added by this story
}
```

### 11.4 Validation Bounds

Add to `pkg/loader/validate.go`:
```go
const (
    maxRobotsGroups         = 50
    maxRobotsUserAgents     = 10  // per group
    maxRobotsUserAgentLen   = 256
    maxRobotsAllowRules     = 50  // per group
    maxRobotsDisallowRules  = 50  // per group
    maxRobotsRuleLen        = 512
    maxPublicBaseURLLen     = 2048
)
```

**`publicBaseURL` validation rules (also reused by E8-S6):**
1. Must be a valid URL parseable by `url.Parse`
2. Must be absolute (`u.IsAbs() == true`)
3. Scheme must be `http` or `https`
4. Must have a non-empty host
5. Must not contain a raw query string or fragment
6. Trailing slash normalized: strip trailing `/` from the path (unless path is `"/"` root)
7. Max length: 2048 bytes

**Robots group validation:**
- At least one `userAgent` per group (empty list is rejected)
- All counts and string lengths within the constants above
- Allow/Disallow values must start with `/` (they are path prefixes, not full URLs)

### 11.5 `publicBaseURL` Normalization

Define and apply normalization at load/validation time so that `https://example.com` and `https://example.com/` are treated identically in the DB and in generated artifacts:

```go
func normalizePublicBaseURL(raw string) (string, error) {
    u, err := url.Parse(strings.TrimSpace(raw))
    if err != nil { ... }
    if u.Scheme != "http" && u.Scheme != "https" { ... }
    if u.Host == "" { ... }
    if u.RawQuery != "" || u.Fragment != "" { ... }
    if u.Path != "/" {
        u.Path = strings.TrimRight(u.Path, "/")
    }
    return u.String(), nil
}
```

Store the normalized form in the DB; the generated `robots.txt` uses the normalized value.

### 11.6 Generation Function (in `internal/release/robots.go`)

```go
// GenerateRobotsText returns a deterministic robots.txt.
// Returns ("", nil) when robots is nil or disabled.
// Accepts an optional sitemapURL for E8-S6 integration; pass "" if sitemap not enabled.
func GenerateRobotsText(robots *model.WebsiteRobots, sitemapURL string) string
```

**Output rules:**
- UTF-8 encoding, LF (`\n`) line endings only — no CRLF
- Groups emitted in input order
- Within each group: `User-agent:` lines first (in input order), then `Allow:` lines, then `Disallow:` lines
- Blank line between groups
- Default allow-all when `enabled == true` and `groups` is empty: `User-agent: *\nAllow: /\n`
- If `sitemapURL != ""`, append `Sitemap: {sitemapURL}` as the final line (after a blank line separator)

### 11.7 Materialization in `builder.go`

**Integration point:** insert after `copyOriginalAssets` and before `materializeOGImages` (or at the equivalent position). Robots.txt is a generated artifact like OG images — it is not in the desired-state input manifest.

```go
func (b *Builder) materializeRobotsFile(
    ctx context.Context,
    seo *model.WebsiteSEO,
    sitemapURL string, // passed from sitemap materialization (E8-S6); empty if disabled
    releaseDir string,
) error
```

The builder reconstructs `site.Website.Spec.SEO` from `WebsiteRow.SeOJSON` during `materializeSource` — the same way it reconstructs page head from `PageRow.HeadJSON`.

### 11.8 State Merge Backward Compatibility

In `internal/state/merge.go`:
- If the apply bundle contains a `Website` resource with a `spec.seo` block, persist it to `seo_json`.
- If the apply bundle contains a `Website` resource **without** a `spec.seo` block, persist `"{}"` to `seo_json` (explicit clear).
- If the apply bundle contains **no** `Website` resource at all, preserve the existing `seo_json` in the DB untouched.

This matches E8-S4's conservative merge behavior for `head_json`.

### 11.9 robots.txt Is Not a Manifest Entry

`robots.txt` is a **generated release artifact** (like rendered HTML and OG images), not a desired-state input. Do not add it to the `.manifest.json` resource list or to the diff manifest. The desired-state diff tracks the `website.yaml` source content hash (which captures the robots config declaratively).

### 11.10 Content-Type and Encoding

- Written to disk as UTF-8 with LF line endings
- When served by Caddy/htmlservd: `Content-Type: text/plain; charset=utf-8`
- No special escaping of user-agent strings or path values — these are rendered verbatim from the validated config

### 11.11 E8-S6 Integration Contract

E8-S6 (sitemap) will pass a `sitemapURL` string into `GenerateRobotsText`. Implement `robots.go` from the start with this signature to avoid a breaking change later. The caller (builder) is responsible for computing `sitemapURL = seo.PublicBaseURL + "/sitemap.xml"` and passing it through only when sitemap is also enabled.

---
